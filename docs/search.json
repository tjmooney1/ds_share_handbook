[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science at SHARE Creative",
    "section": "",
    "text": "Preface to handbook\nThis section will provide an overview of this handbook.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you have any questions at all, ask any member of the team. Whilst this Handbook aims to be a valuable resource for self-learning, it can often be more beneficial to spend 5 minutes talking through a concept with someone on the team who may be able to describe something in a different manner to this document.\n\n\n\n\nHandbook styling guide\nTimbo, we want this Handbook to appear written as a “DS Scientist at SHARE” rather than the Jimbo + Timbo show. To help streamline this, I suggest we follow the following guidelines to help ensure we are consistent with our writing style. Some of these points will not be relevant for all section, but I think we would be wise to always keep them in the back of our mind throughout:\n\nTone, we should be friendly but professional. We can definitely joke, but always try and make sure the point we are getting across is clear.\nLet’s make sure we use the active voice rather than passive voice and talk in the first person plural (i.e. we, us rather than I, me etc).\nIf we are referring to a general data scientist or person, let’s be inclusive and refer to them as “they/them”. For example if we had the sentence “Imagine a data scientist needs to embed a sentence, there are many models available on HuggingFace for them to choose from”.\nWe need to take responsibility for what we write (this will be more relevant in the later sections of the handbook). We need to be make sure we understand concepts before committing them to the handbook- the creation of this handbook will not only help any newcomer get on board quicker, but also help cement DS concepts ourselves.\n\n\n“If you can’t explain it simply, you don’t understand it well enough” - Albert Einstein",
    "crumbs": [
      "Preface to handbook"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "The Data Science team",
    "section": "",
    "text": "Where we sit\nCapture Intelligence is our biggest internal “client” as there are plenty of opportunities to offer data science led services in the research offering of Capture. But also have our own core central pipe for development that supports all agency brands.\nWhat this means is as a team we have responsibilities that range from continual development of our own tech stack to help answer specific research questions for external clients to helping empower members of the alliance to use mindful applications of emerging technologies.",
    "crumbs": [
      "The Data Science team"
    ]
  },
  {
    "objectID": "summary.html#where-we-sit",
    "href": "summary.html#where-we-sit",
    "title": "The Data Science team",
    "section": "",
    "text": "The Data Science department are a fully global resource within the alliance",
    "crumbs": [
      "The Data Science team"
    ]
  },
  {
    "objectID": "summary.html#our-ethos",
    "href": "summary.html#our-ethos",
    "title": "The Data Science team",
    "section": "Our ethos",
    "text": "Our ethos",
    "crumbs": [
      "The Data Science team"
    ]
  },
  {
    "objectID": "project_management.html",
    "href": "project_management.html",
    "title": "1  A Data Science project",
    "section": "",
    "text": "1.1 What is a Data Science project?\nAside from the obvious definition of a project (a piece of work planned and executed to achieve a particular aim- in this case facilitate a client’s needs), what this section is referring to is the structure and usage of a coding project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Data Science project</span>"
    ]
  },
  {
    "objectID": "project_management.html#where-are-projects-savedlocated",
    "href": "project_management.html#where-are-projects-savedlocated",
    "title": "1  A Data Science project",
    "section": "1.2 Where are projects saved/located?",
    "text": "1.2 Where are projects saved/located?\nAll projects need to be saved onto the Google Drive. We have our own Data Science section, where we save our project and internal work (code, data, visualisations etc), which is in the filepath:\nShare_Clients/data_science_project_work/\nYou should get access to this directory straight away.\nWithin the data_science_project_work directory there are subdirectories of all of our clients, such as data_science_project_work/microsoft, data_science_project_work/dyson etc.\n\n\n\nScreenshot of the data_science_project_work/ directory with client-specific subdirectories\n\n\n\n\n\n\n\n\nFile paths\n\n\n\n\n\nYou will see that we refer to the location of directories mainly by their filepath, with the above screenshot of the Google Drive just for full transparency and clarity.\nThere are no two ways about it, getting familiar with working with filepaths in the command line (or in a script) is non-negotiable, but will become second nature and you will be tab-completing filepaths in no time at all!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Data Science project</span>"
    ]
  },
  {
    "objectID": "project_management.html#rstudio-projects",
    "href": "project_management.html#rstudio-projects",
    "title": "1  A Data Science project",
    "section": "1.3 RStudio Projects",
    "text": "1.3 RStudio Projects\nWe are primarily an R focused team, and as such we utilise RStudio projects to help keep all the files associated with a given project together in one directory.\nTo create a RStudio Project, click File &gt; New Project and then follow the below steps, but call the directory the name of the project (if a Microsoft project, appended by the project number) rather than ‘r4ds’. Be sure to make sure the option ‘Create project as subdirectory of’ is the client directory on the Drive (in the case of Microsoft, this is Share_Clients/data_science_project_work/microsoft/project_work/).\n\n\n\nSteps to create a new project, taken from R for Data Science (2e) https://r4ds.hadley.nz/workflow-scripts.html#projects\n\n\nOnce this process is complete, there should be a new project folder in the client directory, with a .Rproj file within it.\nIf this is your first time using RStudio Projects, we recommend reading this section within the R for Data Science book, to familiarise yourself with some more intricacies of Project work within R (such as relative and absolute paths) which we would not do justice summarising here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Data Science project</span>"
    ]
  },
  {
    "objectID": "project_management.html#components-of-a-ds-project",
    "href": "project_management.html#components-of-a-ds-project",
    "title": "1  A Data Science project",
    "section": "1.4 Components of a DS project",
    "text": "1.4 Components of a DS project\nDS projects consist of a parent project directory, with an associated .Rproj file, and three compulsory subdirectories code, data, and viz (all of which are made manually).\n\n\n\n\n\nWhilst there are no prizes for what goes in each subdirectory, it can be useful to have a structure in place to facilitate workflow ease.\n\n1.4.0.1 code\nWithin the code subdirectory is where all scripts should be kept. We utilise .Rmd (R Markdown) documents rather than basic .R scripts for our code.\nWe do this for a few reasons, but the main benefits include:\n\nIt acts as an environment in which to do data science- we can capture not only what we did, but importantly why we did it\nWe can easily build and export a variety of different output formats from a R Markdown document (PDF, HTML, slideshow etc)\n\nAs part of our commitment to literate programming, there are some good practices that we can implement at this level of abstraction.\nFirstly, do not have extremely long .Rmd documents, as this is no good for anybody. Instead split up your documents into different sections based on the purpose of the code.\nWhilst this can be a bit subjective, a good rule of thumb is to have a separate .Rmd for each aspect of a workflow. For example, we might have one .Rmd for reading in raw data, another for cleaning the data, another for EDA, and another for performing topic modelling etc.\nWe should also follow the tidyverse style guide in the naming of files, which states:\n\nIf files should be run in a particular order, prefix them with numbers\n\nTherefore it makes sense to prefix our files, as we must load in the raw data before we can clean the data, and we must clean the data before we can perform certain analyses etc.\nSo we might have something like 00_load_data.Rmd, 01_clean_data.Rmd, 02_topic_modelling.Rmd.\n\n\n1.4.0.2 data\ndata is where we save any data file that comes from a project.\nThe vast majority of projects will involve analysing an export from a social listening platform, such as Sprinklr. Analysts will save the export in the form of .csv or .xlsx files on the Drive (not within the Data Science section). As Sprinklr limits its exports to 10k rows of data per export file, we often are presented with 10s/100s of files with raw mentions. Therefore once we read these files into R, it is a good opportunity to save them as an .Rds in the code folder using the function write_rds() to avoid having to reread the raw excel or csv files in again.\nIt is within data where you would also save cleaned datasets and the outputs of different analyses (not visualisations though). This is not limited to .Rds files, but could also be word documents, excel spreadsheets etc.\nAs projects get more complex with many analyses, it can be easy to clutter this subdirectory. As such, it is recommended to make folders within data to help maintain structure. This means it is easy to navigate where cleaned data is because it will be in a folder such as data/cleaned_data and a dataframe with topic labels would be in data/topic_modelling.\n\n\n\n\n\n\nSave liberally\n\n\n\n\n\nGenerally speaking, space is cheaper than time. If in doubt, save an intermediate dataframe after an analysis if you think you’ll need it in the future. It is better to run an analysis once and save the output to never look at it again, than to run an analysis, not save the output, and then need to rerun the analysis the following week.\n\n\n\n\n\n1.4.0.3 viz\nAny visualisation that is made throughout the project should be saved here. Again, this directory should be split into separate folders to keep different analyses separate, navigable, and clear. This is especially useful if there is are visualisations being made of the same analysis mapped over different variables or parameters, or if the project involves the analysis of separate products or brands.\nFor example, the below shows a screenshot of a viz folder for a project that looked at three products. Within viz the plots for each brand are in their own folder, and within each brand (chatgpt, copilot, gemini) there are further folders to split up the type of visualisations created (area_charts, eda etc), with even a third level of subdirectory (area_charts/peaks and area_charts/pits).\n\n\n\nExample viz directory hierarchy for a Peaks and Pits project",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Data Science project</span>"
    ]
  },
  {
    "objectID": "project_management.html#bonus-for-making-it-this-far",
    "href": "project_management.html#bonus-for-making-it-this-far",
    "title": "1  A Data Science project",
    "section": "1.5 Bonus for making it this far",
    "text": "1.5 Bonus for making it this far\nWe actually have a function that can create an initiate an RStudio project in seconds.\nNeed to speak to Jack about this, to make sure we aren’t crossing paths/confusing anybody\n\nJPackage::folder_setup(project_name = \"00_PROJECT\",\n                       subdir = \"microsoft/project_work\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Data Science project</span>"
    ]
  },
  {
    "objectID": "project_players.html",
    "href": "project_players.html",
    "title": "Project key players",
    "section": "",
    "text": "Insights Analyst\nAnalysts add the bit of human-insight sparkle to our projects. They work closely with the client and stakeholder to help frame our findings so they are suitable for the clients business needs. At a high level, we may say “Our clustering analysis identified five distinct regions of conversation based on the semantics of the language used” whereas an analyst would translate that to “We have five key conversational themes that can be targeted with tailored marketing strategies to boost product reach on socials”. Though this does vary on a project by project basis and we often have to act as a conduit between the science and the client too.\nInsight analysts are who work closely with Sprinklr and other social listening platforms to obtain the data we analyse. They will craft queries to pull the relevant data from a variety of sources and save the data on the Drive for us to access and do science on.\n\n\nAccount Manager\nAccount Managers (AMs) are the point of contact between our business and the client, bridging the gap between the technical teams (in our cases DS or Insights) and the client/stakeholder. They will arrange meetings with the client, help us understand the client’s needs and business objectives, and coordinate project logistics, timelines, and deliverables. As project deadlines approach, account managers will help QA our deliveries (normally in the form of a PowerPoint or Keynote presentation), providing valuable opinion from a non-technical background (it can be easy for us to get stuck in the weeds and forget that stakeholders do not know as much about data as we do). Broadly, AMs make sure both us as a company, and the client, are held accountable for the work we are contracted to do.\n\n\nData/Insight Director\nDepending on the project, there will be a Data Director or Insight Director involved on the project too. You will notice that they will normally be resources on Float Whilst not working on the nitty gritty of the project, they are there to help steer the project in the appropriate direction based on the clients business needs. They will also be checking the final delivery as it is created, making sure the deliverables and story we have thread is suitable and valid.",
    "crumbs": [
      "Project key players"
    ]
  },
  {
    "objectID": "peaks_pits_workflow.html",
    "href": "peaks_pits_workflow.html",
    "title": "2  Peaks and Pits",
    "section": "",
    "text": "2.1 What is the concept/project background?\nStrong memories associated to brands or products go deeper than simple positive or negative sentiment. Most of our experiences are not encoded in memory, rather what we remember about experiences are changes, significant moments, and endings. In their book “The Power of Moments”, two psychologists (Chip and Dan Heath) define these core memories as Peak and Pits, impactful experiences in our lives.\nBroadly, peak moments are experiences that stand our memorable in our lives in a positive sense, whereas pit moments are impactful negative experiences.\nMicrosoft tasked us with finding a way to identify these moments in social data- going beyond ‘simple’ positive and negative sentiment which does not tell the full story of consumer/user experience. The end goal is that by providing Microsoft with these peak and pit moments in the customer experience, they can design peak moments in addition to simply removing pit moments.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Peaks and Pits</span>"
    ]
  },
  {
    "objectID": "peaks_pits_workflow.html#what-is-the-conceptproject-background",
    "href": "peaks_pits_workflow.html#what-is-the-conceptproject-background",
    "title": "2  Peaks and Pits",
    "section": "",
    "text": "2.1.1 The end goal\nWith these projects the core final ‘product’ is a collection of different peaks and pits, with suitable representative verbatims and an explanation to understand the high-level intricacies of these different emotional moments.\n\n\n\nScreenshot from a Peaks and Pits project showcasing the identified Peak moments for a product at a high level\n\n\n\n\n2.1.2 Key features of project\n\nThere is no out-of-the-box ML model available whose purpose is to classify social media posts as either peaks or pits (i.e. we cannot use a ready-made solution, we must design our own bespoke solution).\nThere is limited data available\n\nUnlike the case of spam/ham or sentiment classification, there is not a bank of pre-labelled data available for us to leverage for ‘traditional ML’.\n\nDespite these issues, the research problem itself is well defined (what are the core peak and pit moments for a brand/product), and because there are only three classes (peak, pit, or neither) which are based on extensive research, the classes themselves are well described (even if it is case of “you know a peak moment when you see it”).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Peaks and Pits</span>"
    ]
  },
  {
    "objectID": "peaks_pits_workflow.html#overview-of-approach",
    "href": "peaks_pits_workflow.html#overview-of-approach",
    "title": "2  Peaks and Pits",
    "section": "2.2 Overview of approach",
    "text": "2.2 Overview of approach\nPeaks and pits projects have gone through many iterations throughout the past year and a half. Currently, the general workflow is to use utilise a model framework known as SetFit to efficiently train a text classification model with limited training data. This fine-tuned model is then able to run inference over large datasets to label posts as either peaks, pits, or neither. We then utilise the LLM capabilities to refine these peak and pit moments into a collection of posts we are extremely confident are peaks and/or pits. We then employ topic modelling to identify groups of similar peaks and pits to help us organise and discover hidden topics or themes within this collection of core moments.\nThis whole process can be split into six distinct steps:\n\nExtract brand/product mentions from Sprinklr (the start of any project)\nObtain project-specific exemplar posts to help fine-tune a text classification model\nPerform model fine-tuning through contrastive learning\nRun inference over all of the project specific data\nUse GPT-3.5 for an extra layer of classification on identified peaks and pits\nTurn moments into something interpretable using topic modelling\n\n\n\n\nSchematic workflow from Project 706 - Peaks and Pits in M365 Apps\n\n\n\n2.2.1 Obtain posts for the project (Step 1)\nThis step relies on the analysts to export relevant mentions from Sprinklr (one of the social listening tools that analysts utilise to obtain social data), and therefore is not detailed much here. What is required is one dataset for each of the brands/products, so they can be analysed separately.\n\n\n2.2.2 Identify project-specific exemplar peaks and pits to fine-tune our ML model (Step 2)\nThis step is synonymous with data labelling required for any machine learning project where annotated data is not already available.\nWhilst there is no one-size-fits-all for determining the amount of data required to train a machine learning model, for traditional models and tasks, the number of examples per label is often in the region of thousands, and often this isn’t even enough for more complex problems.\nThe time required to accurately label thousands of peaks and pits to train a classification model in the traditional way is sadly beyond the scope of feasibility for our projects. As such we needed an approach that did not rely on copious amounts of pre-labelled data.\nThis is where SetFit comes in. As mentioned previously, SetFit is a framework that enables us to efficiently train a text classification model with limited training data.\n\n\n\n\n\n\nHow does it do this?\n\n\n\n\n\nNote the below is directly copied from the SetFit documentation. It is so succinctly written that trying to rewrite it would not do it justice.\nEvery SetFit model consists of two parts: a sentence transformer embedding model (the body) and a classifier (the head). These two parts are trained in two separate phases: the embedding finetuning phase and the classifier training phase. This conceptual guide will elaborate on the intuition between these phases, and why SetFit works so well.\nEmbedding finetuning phase\nThe first phase has one primary goal: finetune a sentence transformer embedding model to produce useful embeddings for our classification task. The Hugging Face Hub already has thousands of sentence transformer available, many of which have been trained to very accurately group the embeddings of texts with similar semantic meaning.\nHowever, models that are good at Semantic Textual Similarity (STS) are not necessarily immediately good at our classification task. For example, according to an embedding model, the sentence of 1) “He biked to work.” will be much more similar to 2) “He drove his car to work.” than to 3) “Peter decided to take the bicycle to the beach party!”. But if our classification task involves classifying texts into transportation modes, then we want our embedding model to place sentences 1 and 3 closely together, and 2 further away.\nTo do so, we can finetune the chosen sentence transformer embedding model. The goal here is to nudge the model to use its pretrained knowledge in a different way that better aligns with our classification task, rather than making the completely forget what it has learned.\nFor finetuning, SetFit uses contrastive learning. This training approach involves creating positive and negative pairs of sentences. A sentence pair will be positive if both of the sentences are of the same class, and negative otherwise. For example, in the case of binary “positive”-“negative” sentiment analysis, (“The movie was awesome”, “I loved it”) is a positive pair, and (“The movie was awesome”, “It was quite disappointing”) is a negative pair.\nDuring training, the embedding model receives these pairs, and will convert the sentences to embeddings. If the pair is positive, then it will pull on the model weights such that the text embeddings will be more similar, and vice versa for a negative pair. Through this approach, sentences with the same label will be embedded more similarly, and sentences with different labels less similarly.\nConveniently, this contrastive learning works with pairs rather than individual samples, and we can create plenty of unique pairs from just a few samples. For example, given 8 positive sentences and 8 negative sentences, we can create 28 positive pairs and 64 negative pairs for 92 unique training pairs. This grows exponentially to the number of sentences and classes, and that is why SetFit can train with just a few examples and still correctly finetune the sentence transformer embedding model. However, we should still be wary of overfitting.\nClassifier training phase\nOnce the sentence transformer embedding model has been finetuned for our task at hand, we can start training the classifier. This phase has one primary goal: create a good mapping from the sentence transformer embeddings to the classes.\nUnlike with the first phase, training the classifier is done from scratch and using the labelled samples directly, rather than using pairs. By default, the classifier is a simple logistic regression classifier from scikit-learn. First, all training sentences are fed through the now-finetuned sentence transformer embedding model, and then the sentence embeddings and labels are used to fit the logistic regression classifier. The result is a strong and efficient classifier.\nUsing these two parts, SetFit models are efficient, performant and easy to train, even on CPU-only devices.\n\n\n\nThere is no perfect number of labelled examples to find per class (i.e. peak, pit, or neither). Whilst in general more exemplars (and hence more training data) is beneficial, having fewer but high quality labelled posts is far superior than more posts of poorer quality. This is extremely important due to the contrastive nature of SetFit where it’s superpower is making the most of few, extremely good, labelled data.\nOkay so now we know why we need labelled data, and we know what the model will do with it, what is our approach for obtaining the labelled data?\nBroadly, we use our human judgement to read a post from the current project dataset, and manually label whether we think it is a peak, a pit, or neither. To avoid us just blindly reading through random posts in the dataset in the hope of finding good examples (this is not a good use of time), we can employ a couple of tricks to narrow our search region to posts that have a reasonable likelihood of being suitable examples.\n\nThe first trick is to use the OpenAI API to access a GPT model. This involves taking a sample of posts (say ~1000) and running these through a GPT model, with a prompt that asks the model to classify each post into either a peak, pit, or neither. This is possible because GPT models have learned knowledge of peaks and pits from its training on large datasets. We can therefore get a human to only sense-check/label posts that GPT also believes are peaks or pits.\nThe second trick involves using a previously created SetFit model (i.e. from an older project), and running inference over a similar sample of posts (again, say ~1000).\n\nWe would tend to suggest the OpenAI route, as it is simpler to implement (in our opinion), and often the old SetFit model has been finetuned on data from a different domain so it might struggle to understand domain specific language in the current dataset. However, be aware if it not as scalable as using an old SetFit model and has the drawback of being a prompt based classification of a black-box model (as well as issues relating to cost and API stability).\nIrrespective of which approach is taken, by the end of this step we need to have a list of example posts we are confident represent what a peak or pit moment looks like for each particular product we are researching, including posts that are “neither”.\n\n\n\n\n\n\nWhy do we do this for each project?\n\n\n\n\n\nAfter so many projects now don’t we already have a good idea of what a peak and pit moment for the purposes of model training?\nEach peak and pit project we work on has the potential to introduce ‘domain’ specific language, which a machine learning classifier (model) may not have seen before. By manually identifying exemplar peaks and pits that are project-specific, this gives our model the best chance to identify emotional moments appropriate to the project/data at hand.\nThe obvious case for this is with gaming specific language, where terms that don’t necessarily relate to an ‘obvious’ peak or pit moment could refer to one the gaming conversation, for example the terms/phrases “GG”, “camping”, “scrub”, and “goat” all have very specific meanings in this domain that differ from their use in everyday language.\n\n\n\n\n\n2.2.3 Train our model using our labelled examples (Step 3)\nBefore we begin training our SetFit model with our data, it’s necessary to clean and wrangle the fine-tuning datasets. Specifically, we need to mask any mentions of brands or products to prevent bias. For instance, if a certain brand frequently appears in the training data within peak contexts, the model could erroneously link peak moments to that brand rather than learning the peak-language expressed in the text.\n\nThis precaution should extend to all aspects of our training data that might introduce biases. For example, as we now have examples from various projects, an overrepresentation of data from ‘gaming projects’ in our ‘peak’ posts within our training set (as opposed to the ‘pit’ posts) could skew the model into associating gaming-related language more with peaks than pits.\n\nBroadly the cleaning steps that should be applied to our data for finetuning are:\n\nMask brand/product mentions\nRemove hashtags #️⃣\nRemove mentions 💬\nRemove URLs 🌐\nRemove emojis 🐙\n\n\n\n\n\n\n\nWhat about other cleaning steps?\n\n\n\n\n\nYou will notice here we do not remove stop words-. As explained in the previous step, part of the finetuning process is to finetune a sentence embedding model, and we want to keep stop words so that we can use the full context of the post in order to finetune accurate embeddings.\n\n\n\nAt this step, we can split out our data into training, testing, and validation datasets. A good rule of thumb is to split the data 70% to training data, 15% to testing data, and 15% to validation data. By default, SetFit oversamples the minimum class within the training data, so we shouldn’t have to worry too much about imbalanced datasets- though be aware if we have extreme imbalanced we will end up sampling the same contrastive pairs (normally positive pairs) multiple times. However, our experimentation has shown that class imbalance doesn’t seem to have a significant effect to the training/output of the SetFit model for peaks and pits.\nWe are now at the stage where we can actually fine-tune the model. There are many different parameters we can change when fine-tuning the model, such as the specific embedding model used, the number of epochs to train for, the number of contrastive pairs of sentences to train on etc. For more details, please refer to the Peaks and Pits Playbook\nWe can assess model performance on the testing dataset by looking at accuracy, precision, recall, and F1 scores. For peaks and pits, the most important metric is actually recall because in step 4 we reclassify posts using GPT, so we want to make sure we are able to provide as many true peak/pit moments as possible to this step, even if it means we also provide a few false positives.\n\n\n\n\n\n\nClick here for more info as to why recall is most important\n\n\n\n\n\nAs a refresher, precision is the proportion of positive identifications that are actually correct (it focuses on the correctness of positive predictions) whereas recall is the proportion of actual positives that are identified correctly (it focuses on capturing all relevant instances).\nIn cases where false positives need to be minimised (incorrectly predicting a non-event as an event) we need to prioritise precision - if you’ve built a model to identify hot dogs from regular ol’ doggos, high precision ensures that normal dogs are not misclassified as hot dogs.\nIn cases where false negatives need to be minimised (failing to detect an actual event) we need to prioritise recall - in medical diagnoses we need to minimise the number of times a patient is incorrectly told they do not have a disease when in reality they do (or worded differently, we need to ensure that all patients with a disease are identified).\nTo apply this to our problem- we want to be sure that we capture all (or as many as possible) relevant instances of peaks or pits- even if a few false positives come in (neither posts that are incorrectly classified as peaks or pits). As we use GPT to make further peak/pit identifications, it’s better to provide GPT with with a comprehensive set of potential peaks and pits, including some incorrect ones, than to miss out on critical data.\n\n\n\n\nVisualise model separation\nAs a bonus, we can actually neatly visualise how well our finetuning of the sentence transformer embedding model has gone- by seeing how well the model is able to separate our different classes in embedding space.\nWe can do this by visualising the 2-D structure of the embeddings and see how they cluster:\nThis is what it looks the embeddings space looks like on an un-finetuned model:\n\n\n\nUn-finetuned embedding model\n\n\nHere we can see that posts we know are peaks, pits, and neither all overlap and there is no real structure in the data. Any clustering of points observed are probably due to the posts’ semantic similarity (c.f. the mode of transport example above). We would not be able to nicely use a classifier model to get a good mapping from this embedding space to our classes (i.e. we couldn’t easily separate classes here).\nBy visualising the same posts after finetuning the embedding model, we get something more like this, where we can see that the embedding model now clearly separates posts based on their peak/pit classification (though we must be wary of overfitting!).\n\n\n\nFinetuned embedding model\n\n\nFinally, now we are happy with our model performance based on the training and validation datasets, we can evaluate the performance of this final model using our testing data. This is data that the model has never seen, and we are hoping that the accuracy and performance is similar to that of the validation data. This is Machine Learning 101 and if a refresher is needed for this there are plenty of resources online looking at the role of training, validation, and testing data.\n\n\n\n2.2.4 Run inference over project data (Step 4)\nIt is finally time to infer whether the project data contain peaks or pits by using our fine-tuned SetFit model to classify the posts.\nBefore doing this again we need to make sure we do some data cleaning on the project specific data.\nBroadly, this needs to match the high-level cleaning we did during fine-tuning stage:\n\nMask brand/product mentions (using RoBERTa-based model [or similar] and Rivendell functions)\nRemove hashtags #️⃣\nRemove mentions 💬\nRemove URLs 🌐\nRemove emojis 🐙\n\n\n\n\n\n\n\nNote on social media sources\n\n\n\n\n\nCurrently all peak and pit projects have been done on Twitter or Reddit data, but if a project includes web/forum data quirky special characters, numbered usernames, structured quotes etc. should also be removed.\n\n\n\nOkay now we can finally run inference. This is extremely simple and only requires a couple of lines of code (again see the Peaks and Pits Playbook for code implementation)\n\n\n2.2.5 The metal detector, GPT-3.5 (Step 5)\nDuring step 4 we obtained peak and pit classification using few-shot classification with SetFit. The benefit of this approach (as outlined previously) is its speed and ability to classify with very few labelled samples due to contrastive learning.\nHowever, during our iterations of peak and pit projects, we’ve realised that this step still classifies a fair amount of non-peak and pit posts incorrectly. This can cause noise in the downstream analyses and be very time consuming for us to further trudge through verbatims.\nAs such, the aim of this step is to further our confidence in our final list of peaks and pits to be actually peaks and pits. Remember before we explained that for SetFit, we focussed on recall being the most important measure in our business case? This is where we assume that GPT-3.5 enables us to remove the false positives due to it’s incredibly high performance.\n\n\n\n\n\n\nWhy not use GPT from the start?\n\n\n\n\n\nUsing GPT-3.5 for inference, even over relatively few posts as in peaks and pits, is expensive both in terms of time and money. Preliminary tests have suggested it is in the order of magnitude of thousands of times slower than SetFit. It is for these reasons why we do not use GPT-x models from the get go, despite it’s obvious incredible understanding of natural language.\n\n\n\nWhilst prompt-based classification such as those with GPT-3.5 certainly has its drawbacks (dependency on prompt quality, prompt injections in posts, handling and version control of complex prompts, unexpected updates to the model weights rendering prompts ineffective), the benefits include increased flexibility in what we can ask the model to do. As such, in the absence of an accurate, cheap, and quick model to perform span detection, we have found that often posts identified as peaks/pits did indeed use peak/pit language, but the context of the moment was not related to the brand/product at the core of the research project.\nFor example, take the post that we identified in the project 706, looking for peaks and pits relating to PowerPoint:\n\nThis brings me so much happiness! Being a non-binary graduate student in STEM academia can be challenging at times. Despite using my they/them pronouns during introductions, emails, powerpoint presentations, name tags, etc. my identity is continuously mistaken. Community is key!\n\nThis is clearly a ‘peak’, however it is not accurate or valid to attribute this memorable moment to PowerPoint. Indeed, PowerPoint is merely mentioned in the post, but is not a core driver of the Peak which relates to feeling connection and being part of a community. This is as much a PowerPoint Peak as it is a Peak for the use of emails.\nTherefore, we can engineer our prompt to include a caveat to say that the specific peak or pit moment must relate directly to the brand/product usage (if relevant).\n\n\n2.2.6 Topic modelling to make sense of our data (Step 6)\nNow we have an extremely refined set of posts classified as either peak or pits. The next step is to identify what these moments actually relate to (i.e. identify the topics of these moments through statistical methods).\nTo do this, we employ topic modelling via BERTopic to identifying high-level topics that emerge within the peak and pit conversation. This is done separately for each product and peak/pit dataset (i.e. there will be one BERTopic model for product A peaks, another BERTopic model for product A pits, an additional BERTopic model for product B peaks etc).\nWe implement BERTopic using the R package BertopicR. As there is already good documentation on BertopicR this section will not go into any technical detail in regards to implementation.\nFrom BertopicR. we end up with a topic label for each post in our dataset, meaning we can easily quantify the size of each topics and visualise temporal patterns of topic volume etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Peaks and Pits</span>"
    ]
  },
  {
    "objectID": "ai_landscape_workflow.html",
    "href": "ai_landscape_workflow.html",
    "title": "3  AI Landscape",
    "section": "",
    "text": "Let him cook!\n\n\nTimbo, use this qmd to write everything you need for the AI landscape.\n\n3.0.1 Background of project\nExplain here why Microsoft wanted to perform the project, and the overall goal of the project.\n\n\n3.0.2 Final output of project\nCouple of images of the landscape perhaps- with a summary outlining what was found (don’t worry about specific project details, something like “distinct topics of conversation found by human interpretation of the landscape” is better than “in 726 we found 15 new topics of which conversational AI evolved to encompass the ethics topic” which is too specific)\n\n\n3.0.3 How to get there\nGo through a step by step approach as to how one would go about the project. For now, again don’t worry about details (etc I wouldn’t worry about the particular embedding model here, just say that the posts were embedded using a sentence transformer model etc).\nFeel free to add screenshots of things where needed",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>AI Landscape</span>"
    ]
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "4  Our Packages",
    "section": "",
    "text": "4.1 ParseR\nParseR is the collective name for the techniques SHARE uses for text analysis. It’s primarily based on the tidytext philosophy and the analysis is normally carried out in R.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#connectr",
    "href": "packages.html#connectr",
    "title": "4  Our Packages",
    "section": "4.2 ConnectR",
    "text": "4.2 ConnectR\n\nConnectR is our package for network analysis. It helps the user find important individuals by graphing retweets and important communities by graphing mentions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#segmentr",
    "href": "packages.html#segmentr",
    "title": "4  Our Packages",
    "section": "4.3 SegmentR",
    "text": "4.3 SegmentR\n\nSegmentR is the collective name for the techniques SHARE uses to find latent groups in data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#bertopicr",
    "href": "packages.html#bertopicr",
    "title": "4  Our Packages",
    "section": "4.4 BertopicR",
    "text": "4.4 BertopicR\nBertopicR is our package which allows access to BERTopic’s modelling suite in R via reticulate.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#landscaper",
    "href": "packages.html#landscaper",
    "title": "4  Our Packages",
    "section": "4.5 LandscapeR",
    "text": "4.5 LandscapeR\n\nLandscapeR is our package for exploring text data which has been transformed into a navigable landscape. The package makes use of cutting-edge language models and their dense word embeddings, dimensionality reduction techniques, clustering and/or topic modelling as well as Shiny for an interactive data-exploration & cleaning UI.\nIf the conversation has been mapped appropriately, you will find that mentions close together in the Shiny application/UMAP plot have similar meanings, posts far apart have less similar meanings. This makes it possible to understand and explore thousands, hundreds of thousands, or even millions of posts at a level which was previously impossible.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#limpiar",
    "href": "packages.html#limpiar",
    "title": "4  Our Packages",
    "section": "4.6 LimpiaR",
    "text": "4.6 LimpiaR\n\nLimpiaR is an R library of functions for cleaning & pre-processing text data. The name comes from ‘limpiar’ the Spanish verb’to clean’. Generally when calling a LimpiaR function, you can think of it as ‘clean…’.\nLimpiaR is primarily used for cleaning unstructured text data, such as that which comes from social media or reviews. In its initial release, it is focused around the Spanish language, however, some of its functions are language-ambivalent.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#displayr",
    "href": "packages.html#displayr",
    "title": "4  Our Packages",
    "section": "4.7 DisplayR",
    "text": "4.7 DisplayR\nDisplayR is our package for data visualization, offering a wide array of functions tailored to meet various data visualization needs. This versatile package aims to improve data presentation and communication by providing visually engaging and informative graphics.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "packages.html#helpr",
    "href": "packages.html#helpr",
    "title": "4  Our Packages",
    "section": "4.8 HelpR",
    "text": "4.8 HelpR\n\nHelpR is SHARE’s R package for miscellaneous functions that can come in handy across a variety of workflows.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Our Packages</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "5  Data Cleaning",
    "section": "",
    "text": "5.1 Dataset-level cleaning\nGoal: Ensure the dataset as a whole is relevant and of high quality\nThe main steps that we take for this level of cleaning is spam removal, uninformative content removal and deduplication",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#dataset-level-cleaning",
    "href": "data_cleaning.html#dataset-level-cleaning",
    "title": "5  Data Cleaning",
    "section": "",
    "text": "5.1.1 Spam Removal\nWe use the term “spam” quite loosely in our data pre-processing workflows. Whilst the strict definition of “spam” could be something like “unsolicited, repetitive, unwanted content”, we can think of it more broadly any post that displays irregular posting patterns or is not going to provide analytical value to our research project.\n\n5.1.1.1 Hashtag filtering\nThere are multiple ways we can identify spam to remove it. The simplest is perhaps something like hashtag spamming, where an excessive number of hashtags, often unrelated to the content of the post, can be indicative of spam.\nWe can identify posts like this by counting the number of hashtags, and then filtering out posts that reach a certain (subjective) threshold.\n\ncleaned_data &lt;- data %&gt;% \n  mutate(number_of_hashtags = str_extract_all(message_column, \"#\\\\S+\")) %&gt;% \n  filter(number_of_hashtags &lt; 5)\n\nIn the example above we have set the threshold to be 5 (so any post that has 5 or more hashtags will be removed), however whilst this is a valid starting point, it is highly recommend to treat each dataset uniquely in determining which threshold to use.\n\n\n5.1.1.2 Spam-grams\nOften-times spam can be identified by repetitive posting of the same post, or very similar posts, over a short period of time.\nWe can identify these posts by breaking down posts into n-grams, and counting up the number of posts that contain each n-gram. For example, we might find lots of posts with the 6-gram “Click this link for amazing deals”, which we would want to be removed.\nTo do this, we can unnest our text data into n-grams (where we decide what value of n we want), count the number of times each n-gram appears in the data, and filter out any post that contains an n-gram above this filtering threshold.\nThankfully, we have a function within the LimpiaR package called limpiar_spam_grams() which aids us with this task massively. With this function, we can specify the value of n we want and the minimum number of times an n-gram should occur to be removed. We are then able to inspect the different n-grams that are removed by the function (and their corresponding post) optionally changing the function inputs if we need to be more strict or conservative with our spam removal.\n\nspam_grams &lt;- data %&gt;% \n  limpiar_spam_grams(text_var = message_column,\n                     n_gram = 6,\n                     min_freq = 6)\n\n# see remove spam_grams\nspam_grams %&gt;% \n  pluck(\"spam_grams\")\n\n# see deleted posts\nspam_grams %&gt;% \n  pluck(\"deleted\")\n\n# save 'clean' posts\nclean_data &lt;- spam_grams %&gt;% \n  pluck(\"data\")\n\n\n\n5.1.1.3 Filter by post length\nDepending on the specific research question or analysis we will be performing, not all posts are equal in their analytical potential. For example, if we are investigating what specific features contribute to the emotional association of a product with a specific audience, a short post like “I love product” (three words) won’t provide the level of detail required to answer the question.\nWhile there is no strict rule for overcoming this, we can use a simple heuristic for post length to determine the minimum size a post needs to be before it is considered informative. For instance, a post like “I love product, the features x and y excite me so much” (12 words) is much more informative than the previous example. We might then decide that any post containing fewer than 10 words (or perhaps 25 characters) can be removed from downstream analysis.\nOn the other end of the spectrum, exceedingly long posts can also be problematic. These long posts might contain a lot of irrelevant information, which could dilute our ability to extract the core information we need. Additionally, long posts might be too lengthy for certain pipelines. Many embedding models, for example, have a maximum token length and will truncate posts that are longer than this, meaning we could lose valuable information if it appears at the end of the post. Also, from a practical perspective, longer posts take more time to analyse and require more cognitive effort to read, especially if we need to manually identify useful content (e.g. find suitable verbatims).\n\n# Remove posts with fewer than 10 words\ncleaned_data &lt;- data %&gt;% \n  filter(str_count(message_column, \"\\\\w+\") &gt;= 10)\n\n# Remove posts with fewer than 25 characters and more than 2500 characters\ncleaned_data &lt;- data %&gt;% \n  filter(str_length(message_column) &gt;= 25 & str_length(message_column) &lt;= 2500)\n\n\n\n\n5.1.2 Deduplication\nWhile removing spam often addresses repeated content, it’s also important to handle cases of exact duplicates within our dataset. Deduplication focuses on eliminating instances where entire data points, including all their attributes, are repeated.\nA duplicated data point will not only have the same message_column content but also identical values in every other column (e.g., universal_message_id, created_time, permalink). This is different from spam posts, which may repeat the same message but will differ in attributes like universal_message_id and created_time.\nAlthough the limpiar_spam_grams() function can help identify spam through frequent n-grams, it might not catch these exact duplicates if they occur infrequently. Therefore, it is essential to use a deduplication step to ensure we are not analysing redundant data.\nTo remove duplicates, we can use the distinct() function from the dplyr package, ensuring that we retain only unique values of universal_message_id. This step guarantees that each post is represented only once in our dataset.\n\ndata_no_duplicates &lt;- data %&gt;% \n  distinct(universal_message_id, .keep_all = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#document-level-cleaning",
    "href": "data_cleaning.html#document-level-cleaning",
    "title": "5  Data Cleaning",
    "section": "5.2 Document-level cleaning",
    "text": "5.2 Document-level cleaning\nGoal: Prepare each individual document (post) for text analysis.\nAt a document-level (or individual post level), the steps that we take are more small scale. The necessity to perform each cleaning step will depend on the downstream analysis being performed, but in general the different steps that we can undertake are:\n\n5.2.1 Remove punctuation\nOften times we will want punctuation to be removed before performing an analysis because they tend to not be useful for text analysis. This is particularly the case with more ‘traditional’ text analytics, where an algorithm will assign punctuation marks a unique numeric identify just like a word. By removing punctuation we create a cleaner dataset by reducing noise.\n\n\n\n\n\n\nWarning on punctuation\n\n\n\n\n\nFor more complex models, such as those that utilise word or sentence embeddings, we often keep punctuation in. This is because punctuation is key to understanding a sentences context (which is what sentence embeddings can do).\nFor example, there is a big difference between the sentences “Let’s eat, Grandpa” and “Let’s eat Grandpa”, which is lost if we remove punctuation.\n\n\n\n\n\n5.2.2 Remove stopwords\nStopwords are extremely common words such as “and,” “the,” and “is” that often do not carry significant meaning. In text analysis, these words are typically filtered out to improve the efficiency of text analytical models by reducing the volume of non-essential words.\nRemoving stopwords is particularly useful in our projects for when we are visualising words, such as a bigram network or a WLO plot, as it is more effective if precious informative space on the plots is not occupied by these uninformative terms.\n\n\n\n\n\n\nWarning on stopword removal\n\n\n\n\n\nSimilarly to the removal of punctuation, for more complex models (those that utilise word or sentence embeddings) we often keep stopwords in. This is because these stopwords can be key to understanding a sentences context (which is what sentence embeddings can do).\nFor example, imagine if we removed the stopword “not” from the sentence “I have not eaten pizza”- it would become “I have eaten pizza” and the whole context of the sentence would be different.\nAnother time to be aware of stopwords is if a key term related to a project is itself a stopword. For example, the stopwords list SMART treats the term “one” as a stopword. If we were studying different Xbox products, then the console “Xbox One” would end up being “Xbox” and we would lose all insight referring to that specific model. For this reason it is always worth double checking which stopwords get removed and whether it is actually suitable.\n\n\n\n\n\n5.2.3 Lowercase text\nConverting all text to lowercase standardises the text data, making it uniform. This helps in treating words like “Data” and “data” as the same word, and is especially useful when an analysis requires an understanding of the frequency of a term (we rarely want to count “Data” and “data” as two different things) such as bigram networks.\n\n\n5.2.4 Remove mentions\nMentions (e.g., @username) are specific to social media platforms and often do not carry significant meaning for text analysis, and in fact may be confuse downstream analyses. For example, if there was a username called @I_love_chocolate, upon punctuation remove this might end up confusing a sentiment algorithm. Removing mentions therefore helps in focusing on the actual content of the text.\n\n\n\n\n\n\nRetaining mentions, sometimes\n\n\n\n\n\nWe often perform analyses that involve network analyses. For these, we need to have information of usernames because they appear when users are either mentioned or retweeted. In this case we do not want to remove the @username completely, but rather we can store this information elsewhere in the dataframe.\nHowever, broadly speaking if the goal is to analyse the content/context of a paste, removing mentions is very much necessary.\n\n\n\n\n\n5.2.5 Remove URLs\nURLs in posts often point to external content and generally do not provide meaningful information for text analysis. Removing URLs helps to clean the text by eliminating these irrelevant elements.\n\n\n5.2.6 Remove emojis/special characters\nEmojis and special characters can add noise to the text data. While they can be useful for certain analyses (like emoji-specific sentiment analysis - though we rarely do this), they are often removed to simplify text and focus on word-based analysis.\n\n\n5.2.7 Stemming/Lemmatization\nStemming and lemmatization are both techniques used to reduce words to their base or root form and act as a text normalisation technique.\nStemming trims word endings to their most basic form, for example changing “clouds” to “cloud” or “trees” to “tree”. However, sometimes stemming reduces words to a form that doesn’t make total sense such as “little” to “littl” or “histories” to “histori”.\nLemmatization considers the context and grammatical role when normalising words, producing dictionary definition version of words. For example “histories” would become “history”, and “caring” would become “car” (whereas for stemming it would become “car”).\nWe tend to use lemmatization over stemming- despite it being a bit slower due to a more complex model, the benefit of lemmitization outweighs this. Similar to lowercasing the text, lemmitization is useful when we need to normalise text where having distinct terms like “change”, “changing”, “changes”, and “changed” isn’t necessary and just “change” is suitable.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "data_cleaning.html#conclusion",
    "href": "data_cleaning.html#conclusion",
    "title": "5  Data Cleaning",
    "section": "5.3 Conclusion",
    "text": "5.3 Conclusion\nDespite all of these different techniques, it is important to remember these are not mutually exclusive, and do not always need to be performed. It may very well be the case where a specific project actually required us to mine through the URLs in social posts to see where users a linking too, or perhaps keeping text as all-caps is important for how a specific brand or product is mentioned online. Whilst we can streamline the cleaning steps by using the ParseR function above, it is always worth spending time considering the best cleaning steps for each specific part of a project. It is much better spending more time at the beginning of the project getting this right, than realising that the downstream analysis are built on dodgy foundations and the data cleaning step needs to happen again later in the project, rendering intermediate work redundant.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Cleaning</span>"
    ]
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "SHARE glossary\nDeck: A marketing term used to refer to a slideshow presentation of some kind (i.e. a PowerPoint or Keynote)\nSIP: Social Intelligence Practice- the team at Microsoft we most closely work with on projects. They sit within Microsoft’s market research organisation and partner with many wider teams across Microsoft\nQBR: Quarterly Business Review- a quarterly meeting where goals, outcomes, and development are reviewed. You will see this term used both for our own company QBRs, but also QBRs for clients too.\n\n\nData Science glossary\nEmbedding: Numerical representations of real-world objects (in the form of a vector of real numbers)\nEmbedding Model: A model that can perform embedding by encapsulating information into dense representations in a multi-dimensional space\nF1 Score: The harmonic mean of precision and recall (enables us to integrate precision and recall into a single metric)\n\\[\nF1 \\space Score = 2 \\times \\dfrac{Precision \\times Recall}{Precision + Recall}\n\\]\nPrecision: The proportion of positive identifications that were actually correct. Precision focuses on the correctness of positive predictions\n\\[\nPrecision = \\dfrac{TP}{(TP + FP) }\n\\]\nRecall: The proportion of actual positives that were identified correctly. Recall focuses on capturing all relevant instances\n\\[\nRecall = \\dfrac{TP}{(TP + FN) }\n\\]\nSentence Embedding: A numerical representation of a sentence\nToken: The ‘building blocks’ of text that are readable by a model. Depending on the specific model, a token may be a single word, a character, phrases, or parts of words.\nTokenization: The act of separating a piece of text into smaller units known as tokens.\nWord Embedding: A numerical representation of a single word",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "conversation_landscape.html",
    "href": "conversation_landscape.html",
    "title": "3  Conversation Landscape",
    "section": "",
    "text": "3.0.1 Project Background\nWorking with semi-structured or unstructured high-dimensional data, such as text (and in our case, social media posts), poses significant challenges in measuring or quantifying the language used to describe any specific phenomena. One common approach to quantifying language is topic modelling, where a corpus (or collection of documents) is processed and later represented in neater and simplified format. This often involves displaying top terms, verbatims, or threads highlighting any nuances or differences within the data. Traditional topic modelling or text analysis methods, such as Latent Dirichlet Allocation (LDA), operate on the probability or likelihood of terms or n-grams belonging to a set number of topics.\nThe Conversation Landscape workflow offers a slightly different solution and one that partitions text data without a specific need for burdening the user with sifting through rows of data in order to segment documents with hopes of understanding or recognising any differences in language, which would ideally be defined more simply as topics. The is mostly achieved through sentence transforming, where documents are converted from words to numerical values, which are often referred to as ‘embeddings’. These values are calculated based on their content’s semantic and syntactic properties. The transformed values are then processed again using dimension reduction techniques, making the data more suitable for visualisation. Typically, this involves reducing to two dimensions, though three dimensions may be used to introduce another layer of abstraction between our data points. The example provided throughout this chapter, represents some text data as nodes upon a two-dimensional space.\nNote: This documentation will delve deeper into the core concepts of sentence transforming and dimension reduction, along with the different methods used to cluster or group topics once the overall landscape is mapped out, referring back to our illustrated real-world business use case of these techniques. We will then later look at best practices and any downstream flourishes that will help us operate within this work-stream.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>load libraries</span>"
    ]
  },
  {
    "objectID": "conversation_landscape.html#how-to-get-there",
    "href": "conversation_landscape.html#how-to-get-there",
    "title": "3  Conversation Landscape",
    "section": "3.1 How to get there",
    "text": "3.1 How to get there\nAs promised, we will provide some more context as well as the appropriate information surrounding the required steps taken, so that a reader may replicate and implement the methods mentioned throughout so far, providing an efficient analysis tool to use for any set of documents, regardless of domain specifics. While the example output provided displays a simplified means for visualising complex and multifaceted noisy data such as the ‘Artificial Intelligence’ conversation on social, there are a number of steps that one must take carefully and be mindful of throughout, in order to create the best fit model appropriate for a typical Conversation Landscape project.\nThe broad steps would include, and as one might find across many projects within the realms of Natural Language Processing (NLP):\n\nInitial Exploratory Data Analysis (EDA): Checking that the data is relevant and fit to answer the brief.\nCleaning and Processing: Removal of spam, unhelpful or irrelevant data, and pre-processing of text variable for embedding.\nTransforming/Embedding: Turning our words into numbers which will later be transformed again before being visualised.\nDimension Reduction: Reducing our representational values of documents to a manageable state in order to visualise.\nTopic Modelling/Clustering: Scientifically modelling and defining our data into a more digestible format.\n\n\n3.1.0.1 Exploratory Data Analysis (EDA):\nWhether the user is responsible for data querying/collection or not, the first steps in our workflow should always involve some high-level checks before we proceed with any of the following steps in order to save time downstream and give us confidence to carry over into the data cleaning and processing steps and beyond.\nFirst, one should always check things like the existing variables and clean or rename any where necessary. This step requires a little forward thinking as to what columns are necessary to complete each stage of the project. Once happy with our variables, we can then check for things such as missing dates, and/or if there are any abnormal distributions across columns like ‘Social Platform’ that might skew any findings or help us understand or perhaps justify the resulting topic model. Next, we can do some bespoke or project specific checks like searching for likely-to-find terms or strings within our text variable to ensure that the data is relevant and query has captured the phenomena we are aiming to model.\n\n\n3.1.0.2 Data Cleaning/Processing:\nAgain, as we may not always be responsible for data collection, we can expect that our data may contain unhelpful or even problematic information which is often the result of data being unwillingly bought in by the query. Our job at this stage is to minimize the amount of unhelpful data existing in our corpus to ensure our findings are accurate as well as appropriate for the data which we will be modelling.\nOptimal procedures for spam detection and removal are covered in more detail [here]will include link when data cleaning section is complete. However, there are steps the user absolutely must take to ensure that the text variable which will be provided to the sentence transformer model is clean and concise so that an accurate embedding process can take place upon our documents. This includes the removal of:\n\nHashtags #️⃣\nUser/Account Mentions 💬\nURLs or Links 🌐\nEmojis 🐙\nNon-English Characters 🉐\n\nOften, we might also choose to remove punctuation and/or digits, however in our provided example, we have not done so. There are also things to beware of such as documents beginning with numbers that can influence the later processes, so unless we deem them necessary we should remove these where possible to ensure no inappropriate grouping of documents takes place based on these minor similarities. This is because when topic modelling, we aim to capture the pure essence of clusters which is ultimately defined by the underlying semantic meaning of documents, as apposed to any similarities across the chosen format of said documents.\n\n\n3.1.0.3 Sentence Transforming/Embedding:\nOnce we are happy with the cleanliness and relevance of our data, including the processing steps we have taken with our chosen text variable, we can begin embedding our documents so that we have a numerical representation that can later be reduced and visualised for each. Typically, and in this case we have used already pre-trained sentence transformer models that are hosted on Hugging Face, such as all-mpnet-base-v2 which is the specific model we had decided to use in our AI Conversation Landscape example. This is because during that time, the model had boasted great performance scores for how lightweight it was, however with models such as these being open-source, community-lead contributions are made to further train and improve model performance which means that these performance metrics are always increasing, so one may wish to consult the Hugging Face leaderboard, or simply do some desk research before settling on an ideal model appropriate for their own specific use case.\nWhile the previous steps taken might have involved using R and Rstudio and making use of SHARE’s suite of data cleaning, processing and parsing functionality, the embedding process will need to be completed using Google Colab. This is to take advantage of their premium GPUs and high RAM option, as embedding documents can require large amounts of compute, so much so that most fairly competent machines with standard tech specs will struggle. It is also worth noting that an embedding output may depend on the specific GPU being utilized as well as the version of Python that Colab is currently running, it’s good practice to make note of both of these specifics, along with other modules and library versions that one may wish to use in the same session, such as umap-learn (you may thank yourself at a later stage for doing so). To get going with sentence transformers and for downloading/importing a model such as all-mpnet-bas-v2, there are step-by-step guides purposed to enable users with the know-how to use them and deal with model outputs upon the Hugging Face website.\n\n\n3.1.0.4 Dimension Reduction:\nAt this stage, we would expect to have our data cleaned along with the representative embeddings for each document, which is output by the sentence transforming process. This next step, explains how we take this high-dimensional embeddings object and then simplify/reduce columns down enough to a more manageable size in order to map our documents onto a semantic space. Documents can then be easily represented as a node and are positioned within this abstract space based upon their nature, meaning that those more semantically similar will be situated closer together upon our two (or sometimes three-dimensional) plot, which then forms our landscape.\nThere are a number of ways the user can process an embeddings output. Each method has its own merits as well as appropriate use cases, which mostly depend whether the user intends to focus on either the local or global structure of their data. For more on the alternative dimension reduction techniques, the BERTopic documentation provides some further detail while staying relevant to the subject matter of Topic Modelling and NLP.\nOnce we have reduced our embeddings, and for the sake of staying consistent to the context of our given example, lets say we have decided to use Uniform Manifold Approximation and Projection (UMAP), a technique which is helpful for when we wish to represent both the local and global structures of our data. The output of this step should have resulted in taking our high dimensional embedding data (often 768 columns or sometimes more) and reduced these values down to just 2 columns so that we can plot them onto our semantic space (our conversation landscape plot), using these 2 reduced values as if to serve as X and Y coordinates to appropriately map each data point, we often name these two columns V1 and V2.\nAt this stage, we can use the LandscapeR package to render a static visualisation of the entire landscape, and we can select the desired colour of our nodes by making use of the fill_colour parameter. In this instance, we’ve mapped our documents onto the semantic space represented as nodes using columns V1 and V2 and coloured them a dark grey.\n\ndata %&gt;% \n  LandscapeR::ls_plot_static(x_var = V1,\n                             y_var = V2,\n                             fill_colour = \"#808080\")\n\n\n\n\nGrey Colourless Landscape Plot from an AI Landscape Microsoft Project - Q2 FY24\n\n\nIt’s worth pointing out, that there are a number of ways for the user to interactively explore the landscape at this stage by scanning over each node, checking the documents contents. This helps the user to familiarise with each region of the landscape before clustering. The plotly package serves as a user friendly means for this purpose, helping us gather a ‘lay of the land’ and identify the dense and not so dense sections of our data.\nNOTE: This shows just a 20K sample from our data, which is done only to comply with data size limits on GitHub and to be more conservative with compute and memory usage. Here, we also use a message column with breaks every 10 words to ensure the visual is neater\n\ndata %&gt;% plotly::plot_ly(\n  x = ~V1,\n  y = ~V2,\n  type = 'scatter',\n  mode = 'markers',\n  marker = list( color = '#808080', size = 1),\n  text = ~paste('Message: ', message_with_breaks),\n  hoverinfo = 'text'\n)\n\n\n\n\n\n\n\n3.1.0.5 Topic Modelling/Clustering:\nThe final steps taken are arguably the most important, this is where we will define our documents and simplify our findings byway of scientific means, in this case using Topic Modelling.\nThere are a number of algorithms that serve this purpose, but the more commonly used clustering techniques are KMeans and HDBSCAN. However, the example we have shown uses KMeans, where we define the number of clusters that we would expect to find beforehand and perform clustering on either the original embeddings object output by the sentence transformer model, or we can reduce those embeddings to something much smaller like 10 dimensions and cluster documents based on those. If we were to opt for HDBSCAN however, we would allow the model to determine how many clusters were formed based on some input parameters such as min_cluster_size which are provided by the user. For more on these two techniques and when/how to use them in a topic modelling setting, we can consult the BERTopic documentation once more.\nIt’s also worth noting that this step requires a significant amount of human interpretation, so the user can definitely expect to partake in an iterative process of trial and error, trying out different values for the clustering parameters which determine the models output, with hopes of finding the model of best fit, which they feel accurately represents the given data.\nIn practise, this visualisation can be derived using our original data object with topic/cluster information appended, as well as the original V1 and V2 coordinates that we had used previously. To ensure our topics are coloured appropriately, we can create and use a named character vector and some additional ggplot2 syntax to manually assign topics with specific hex codes or colours.\n\n# assign colours to topics\ntopic_colours &lt;- c(\"Ethics & Regulation\" = \"#C1E1C1\",\n                   \"Technological Innovations\" = \"#6e88db\", \n                   \"AI in an Artistic Domain\" = \"#7e2606\",\n                   \"Cultural & Social Impact\" = \"#ff6361\",\n                   \"Business & Wider-Markets\" = \"#063852\",\n                   \"Future of Learning & Personal Growth\" = \"#ffa600\",\n                   \"Future of Work & Security\" = \"#9e1ad6\"\n                   )\n\n\ndata %&gt;% \n  LandscapeR::ls_plot_group_static(x_var = V1,\n                                   y_var = V2,\n                                   group_var = topic_name) +\n  ggplot2::scale_colour_manual(values = topic_colours) # colour nodes manually\n\n\n\n\nSegmented Colourful Landscape Plot from an AI Landscape Microsoft Project - Q2 FY24",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>load libraries</span>"
    ]
  },
  {
    "objectID": "conversation_landscape.html#downstream-flourishes",
    "href": "conversation_landscape.html#downstream-flourishes",
    "title": "3  Conversation Landscape",
    "section": "3.2 Downstream Flourishes",
    "text": "3.2 Downstream Flourishes\nWith the basics of each step covered, we will now touch on a few potentially beneficial concepts worth grasping that may help us overcome anything else that may occur when working within the Conversation Landscape project domain.\n\n3.2.0.1 Model Saving & Reusability:\nOccasionally, a client may want us to track topics over time or perform a landscape change analysis. In these cases, we need to save both our Dimension Reduction and Clustering models so that new data can be processed using these models, tp produce consistent and comparable results.\nThis requires careful planning. When we initially reduce embeddings and perform clustering, we use the .fit() method from sklearn when either reducing the dimensions of or clustering on the original embeddings. This ensures that the models are trained on the data they are intended to represent, making future outputs comparable.\nWe had earlier, mentioned, that it is crucial to document the versions of the modules and Python interpreter used. When we reduce or cluster new data using our pre-fitted models, it is essential to do so with the exact same versions of important libraries and Python. The reason is that the internal representations and binary structures of the models can differ between versions. If we attempt to load and apply previously saved models with different versions, we risk encountering incompatibility errors. By maintaining version control and documenting the environment in which the models were created, we can ensure the reusability of our models. Overall, this practice allows for us to be accurate when tracking and comparing topics and noting any landscape changes.\n\n\n3.2.0.2 Efficient Parameter Tuning:\nWhen we’re performing certain steps within this workflow, more specifically the Dimension Reduction with likes of UMAP, or if we were to decide we’d want to cluster using HDBSCAN for example, being mindful of and efficient with tuning the different parameters at each step will definitely improve the outcome of our overall model. Therefore, understanding these key parameters and how they can interact will significantly enhance the performance of the techniques being used here.\n\n3.2.0.2.1 Dimension Reduction with UMAP:\nn_neighbors: This parameter controls the local neighborhood size used in UMAP. A smaller value focuses more on capturing the local structure, while a larger value considers more global aspects. Efficiently tuning this parameter involves considering the nature of your data and the scale at which you want to observe patterns.\nmin_dist: The min distance argument determines quite literally how tight our nodes are allowed to be positioned together within our semantic space, a lower value for this will mean nodes will be tightly packed together, whereas a higher number will ensure larger spacing of data points.\nn_components: Here is where we decide how many dimensions we wish to reduce our high-dimensional embeddings object down to, for visualisation we will likely set this parameter to a value of 2.\n\n\n3.2.0.2.2 KMeans CLustering\nn_clusters: KMeans is a relatively simple algorithm compared to other methods and components, requiring very little input. Here we just provide a value for the number of clusters we wish to form, this will either be clusters in the embeddings or a smaller, more manageable reduced embeddings object as mentiioned previously.\n\n\n3.2.0.2.3 HDBSCAN Clustering:\nmin_samples: This parameter defines the minimum number of points required to form a dense region. It helps determine the density threshold for clusters and can determine how conservative we want the clustering model to be. Put simply, a higher value can lead to fewer, larger clusters, while a lower value can result in more, smaller clusters.\nmin_cluster_size: This parameter sets the minimum size of clusters. Like min_samples it can directly influence the granularity of the clustering results. In this case, smaller values allow the formation of smaller clusters, while larger values prevent the algorithm from identifying any small clusters(or those below the size of the provided value). It’s worth noting that the relationship between min_samples and min_cluster_size is crucial. min_samples should generally be less than or equal to min_cluster_size. Adjusting these parameters in tandem helps us to control the sensitivity of HDBSCAN, and for us to define what qualifies as a cluster.\n\n\n3.2.0.2.4 Tip: Try starting with the default value for all of these parameters, and incrementally adjust based on the desired granularity or effect of any that we wish to amend.\n\n\n\n3.2.0.3 Supporting Data Visualisation:\nOnce we have our landscape output, as shown in Final output of project, we will inevitably need to display some further information regarding our topics, most commonly; Volume over Time (VOT) and Sentiment Distribution for each.\nWhen doing so, we would ideally keep some formatting consistencies when plotting, as we mentioned previously, the colouring of our topics must remain the same throughout so that they match up with any previous representations in existing visuals such as the landscape output. We would also want to ensure that any plot we create orders our topics by volume or at least in the same order throughout our project. We can order our topics in terms of volume easily with just a few lines of code.\nFirst, we’ll make sure to set the factor levels of our topics by using dplyr::count() on the topic_name column, and setting the levels feature of the factor() base function based on the counted output.\n\n# sort topics by order of volume\ntopic_order &lt;- data %&gt;% dplyr::count(topic_name, sort = TRUE)\n# set levels determined by volume of topic, this orders the group variable for plotting\ndata &lt;- data %&gt;% \n  dplyr::mutate(topic_name = factor(topic_name, levels = topic_order$topic_name))\n\n\n3.2.0.3.1 Topic Volume over Time\nStarting with volume over time, we often choose to render a faceted plot that includes all topics and their VOT for comparison. We can do so by using functionality found in packages such as JPackage for this.\n\n# plot topic volume over time using 'plot_group_vol_time()' function\ndata %&gt;% \n  JPackage::plot_group_vol_time(group_var = topic_name,\n                                date_var = date,\n                                unit = \"day\",\n                                nrow = 2) +\n  ggplot2::scale_fill_manual(values = topic_colours) # apply colours manually!\n\n\n\n\n\n\n\n\n\n\n3.2.0.3.2 Topic Sentiment Distribution\nNext, we might want/need to break each of our topics out by their sentiment distribution to help shine light on any of particular interest or to help us tell a more refined story using our topic model. This can be done by using the dr_plot_sent_group() function of the DisplayR package.\n\ndata %&gt;% \n  DisplayR::dr_plot_sent_group(group_var = topic_name,\n                               sentiment_var = sentiment,\n                               \"percent\", bar_labels = \"none\", \n                               sentiment_colours = c(\"POSITIVE\" = \"darkgreen\",\n                                                     \"NEGATIVE\" = \"darkred\"))\n\n\n\n\n\n\n\n\n\n\n3.2.0.3.3 Alternative Visualisations\nWhile the two visuals we have displayed so far are relatively basic and commonly used, this does not mean that we won’t require alternative methods to display topic-level information. Often, we may render n-grams per topic to display the relationships that exist between terms/phrases, and we may create plots to showcase things such as data source or social network/platform distributions across topics.\nFinally, it’s worth noting that the need for specific data visualisation methods entirely depends on the project domain and brief, as well as any outcomes/findings derived throughout. This means we ought to be flexible in our approach to utilising any technique that may assist with strengthening our understanding of the data and/or supporting our analyses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>load libraries</span>"
    ]
  }
]