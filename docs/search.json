[
  {
    "objectID": "peaks_pits_workflow.html",
    "href": "peaks_pits_workflow.html",
    "title": "2¬† Peaks and Pits",
    "section": "",
    "text": "2.1 What is the concept/project background?\nStrong memories associated to brands or products go deeper than simple positive or negative sentiment. Most of our experiences are not encoded in memory, rather what we remember about experiences are changes, significant moments, and endings. In their book ‚ÄúThe Power of Moments‚Äù, two psychologists (Chip and Dan Heath) define these core memories as Peak and Pits, impactful experiences in our lives.\nBroadly, peak moments are experiences that stand our memorable in our lives in a positive sense, whereas pit moments are impactful negative experiences.\nMicrosoft tasked us with finding a way to identify these moments in social data- going beyond ‚Äòsimple‚Äô positive and negative sentiment which does not tell the full story of consumer/user experience. The end goal is that by providing Microsoft with these peak and pit moments in the customer experience, they can design peak moments in addition to simply removing pit moments.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Peaks and Pits</span>"
    ]
  },
  {
    "objectID": "peaks_pits_workflow.html#what-is-the-conceptproject-background",
    "href": "peaks_pits_workflow.html#what-is-the-conceptproject-background",
    "title": "2¬† Peaks and Pits",
    "section": "",
    "text": "2.1.1 The end goal\nWith these projects the core final ‚Äòproduct‚Äô is a collection of different peaks and pits, with suitable representative verbatims and an explanation to understand the high-level intricacies of these different emotional moments.\n\n\n\nScreenshot from a Peaks and Pits project showcasing the identified Peak moments for a product at a high level\n\n\n\n\n2.1.2 Key features of project\n\nThere is no out-of-the-box ML model available whose purpose is to classify social media posts as either peaks or pits (i.e.¬†we cannot use a ready-made solution, we must design our own bespoke solution).\nThere is limited data available\n\nUnlike the case of spam/ham or sentiment classification, there is not a bank of pre-labelled data available for us to leverage for ‚Äòtraditional ML‚Äô.\n\nDespite these issues, the research problem itself is well defined (what are the core peak and pit moments for a brand/product), and because there are only three classes (peak, pit, or neither) which are based on extensive research, the classes themselves are well described (even if it is case of ‚Äúyou know a peak moment when you see it‚Äù).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Peaks and Pits</span>"
    ]
  },
  {
    "objectID": "peaks_pits_workflow.html#overview-of-approach",
    "href": "peaks_pits_workflow.html#overview-of-approach",
    "title": "2¬† Peaks and Pits",
    "section": "2.2 Overview of approach",
    "text": "2.2 Overview of approach\nPeaks and pits projects have gone through many iterations throughout the past year and a half. Currently, the general workflow is to use utilise a model framework known as SetFit to efficiently train a text classification model with limited training data. This fine-tuned model is then able to run inference over large datasets to label posts as either peaks, pits, or neither. We then utilise the LLM capabilities to refine these peak and pit moments into a collection of posts we are extremely confident are peaks and/or pits. We then employ topic modelling to identify groups of similar peaks and pits to help us organise and discover hidden topics or themes within this collection of core moments.\nThis whole process can be split into six distinct steps:\n\nExtract brand/product mentions from Sprinklr (the start of any project)\nObtain project-specific exemplar posts to help fine-tune a text classification model\nPerform model fine-tuning through contrastive learning\nRun inference over all of the project specific data\nUse GPT-3.5 for an extra layer of classification on identified peaks and pits\nTurn moments into something interpretable using topic modelling\n\n\n\n\nSchematic workflow from Project 706 - Peaks and Pits in M365 Apps\n\n\n\n2.2.1 Obtain posts for the project (Step 1)\nThis step relies on the analysts to export relevant mentions from Sprinklr (one of the social listening tools that analysts utilise to obtain social data), and therefore is not detailed much here. What is required is one dataset for each of the brands/products, so they can be analysed separately.\n\n\n2.2.2 Identify project-specific exemplar peaks and pits to fine-tune our ML model (Step 2)\nThis step is synonymous with data labelling required for any machine learning project where annotated data is not already available.\nWhilst there is no one-size-fits-all for determining the amount of data required to train a machine learning model, for traditional models and tasks, the number of examples per label is often in the region of thousands, and often this isn‚Äôt even enough for more complex problems.\nThe time required to accurately label thousands of peaks and pits to train a classification model in the traditional way is sadly beyond the scope of feasibility for our projects. As such we needed an approach that did not rely on copious amounts of pre-labelled data.\nThis is where SetFit comes in. As mentioned previously, SetFit is a framework that enables us to efficiently train a text classification model with limited training data.\n\n\n\n\n\n\nHow does it do this?\n\n\n\n\n\nNote the below is directly copied from the SetFit documentation. It is so succinctly written that trying to rewrite it would not do it justice.\nEvery SetFit model consists of two parts: a sentence transformer embedding model (the body) and a classifier (the head). These two parts are trained in two separate phases: the embedding finetuning phase and the classifier training phase. This conceptual guide will elaborate on the intuition between these phases, and why SetFit works so well.\nEmbedding finetuning phase The first phase has one primary goal: finetune a sentence transformer embedding model to produce useful embeddings for our classification task. The Hugging Face Hub already has thousands of sentence transformer available, many of which have been trained to very accurately group the embeddings of texts with similar semantic meaning.\nHowever, models that are good at Semantic Textual Similarity (STS) are not necessarily immediately good at our classification task. For example, according to an embedding model, the sentence of 1) ‚ÄúHe biked to work.‚Äù will be much more similar to 2) ‚ÄúHe drove his car to work.‚Äù than to 3) ‚ÄúPeter decided to take the bicycle to the beach party!‚Äù. But if our classification task involves classifying texts into transportation modes, then we want our embedding model to place sentences 1 and 3 closely together, and 2 further away.\nTo do so, we can finetune the chosen sentence transformer embedding model. The goal here is to nudge the model to use its pretrained knowledge in a different way that better aligns with our classification task, rather than making the completely forget what it has learned.\nFor finetuning, SetFit uses contrastive learning. This training approach involves creating positive and negative pairs of sentences. A sentence pair will be positive if both of the sentences are of the same class, and negative otherwise. For example, in the case of binary ‚Äúpositive‚Äù-‚Äúnegative‚Äù sentiment analysis, (‚ÄúThe movie was awesome‚Äù, ‚ÄúI loved it‚Äù) is a positive pair, and (‚ÄúThe movie was awesome‚Äù, ‚ÄúIt was quite disappointing‚Äù) is a negative pair.\nDuring training, the embedding model receives these pairs, and will convert the sentences to embeddings. If the pair is positive, then it will pull on the model weights such that the text embeddings will be more similar, and vice versa for a negative pair. Through this approach, sentences with the same label will be embedded more similarly, and sentences with different labels less similarly.\nConveniently, this contrastive learning works with pairs rather than individual samples, and we can create plenty of unique pairs from just a few samples. For example, given 8 positive sentences and 8 negative sentences, we can create 28 positive pairs and 64 negative pairs for 92 unique training pairs. This grows exponentially to the number of sentences and classes, and that is why SetFit can train with just a few examples and still correctly finetune the sentence transformer embedding model. However, we should still be wary of overfitting.\nClassifier training phase Once the sentence transformer embedding model has been finetuned for our task at hand, we can start training the classifier. This phase has one primary goal: create a good mapping from the sentence transformer embeddings to the classes.\nUnlike with the first phase, training the classifier is done from scratch and using the labelled samples directly, rather than using pairs. By default, the classifier is a simple logistic regression classifier from scikit-learn. First, all training sentences are fed through the now-finetuned sentence transformer embedding model, and then the sentence embeddings and labels are used to fit the logistic regression classifier. The result is a strong and efficient classifier.\nUsing these two parts, SetFit models are efficient, performant and easy to train, even on CPU-only devices.\n\n\n\nThere is no perfect number of labelled examples to find per class (i.e.¬†peak, pit, or neither). Whilst in general more exemplars (and hence more training data) is beneficial, having fewer but high quality labelled posts is far superior than more posts of poorer quality. This is extremely important due to the contrastive nature of SetFit where it‚Äôs superpower is making the most of few, extremely good, labelled data.\nOkay so now we know why we need labelled data, and we know what the model will do with it, what is our approach for obtaining the labelled data?\nBroadly, we use our human judgement to read a post from the current project dataset, and manually label whether we think it is a peak, a pit, or neither. To avoid us just blindly reading through random posts in the dataset in the hope of finding good examples (this is not a good use of time), we can employ a couple of tricks to narrow our search region to posts that have a reasonable likelihood of being suitable examples.\n\nThe first trick is to use the OpenAI API to access a GPT model. This involves taking a sample of posts (say ~1000) and running these through a GPT model, with a prompt that asks the model to classify each post into either a peak, pit, or neither. This is possible because GPT models have learned knowledge of peaks and pits from its training on large datasets. We can therefore get a human to only sense-check/label posts that GPT also believes are peaks or pits.\nThe second trick involves using a previously created SetFit model (i.e.¬†from an older project), and running inference over a similar sample of posts (again, say ~1000).\n\nWe would tend to suggest the OpenAI route, as it is simpler to implement (in our opinion), and often the old SetFit model has been finetuned on data from a different domain so it might struggle to understand domain specific language in the current dataset. However, be aware if it not as scalable as using an old SetFit model and has the drawback of being a prompt based classification of a black-box model (as well as issues relating to cost and API stability).\nIrrespective of which approach is taken, by the end of this step we need to have a list of example posts we are confident represent what a peak or pit moment looks like for each particular product we are researching, including posts that are ‚Äúneither‚Äù.\n\n\n\n\n\n\nWhy do we do this for each project?\n\n\n\n\n\nAfter so many projects now don‚Äôt we already have a good idea of what a peak and pit moment for the purposes of model training?\nEach peak and pit project we work on has the potential to introduce ‚Äòdomain‚Äô specific language, which a machine learning classifier (model) may not have seen before. By manually identifying exemplar peaks and pits that are project-specific, this gives our model the best chance to identify emotional moments appropriate to the project/data at hand.\nThe obvious case for this is with gaming specific language, where terms that don‚Äôt necessarily relate to an ‚Äòobvious‚Äô peak or pit moment could refer to one the gaming conversation, for example the terms/phrases ‚ÄúGG‚Äù, ‚Äúcamping‚Äù, ‚Äúscrub‚Äù, and ‚Äúgoat‚Äù all have very specific meanings in this domain that differ from their use in everyday language.\n\n\n\n\n\n2.2.3 Train our model using our labelled examples (Step 3)\nBefore we begin training our SetFit model with our data, it‚Äôs necessary to clean and wrangle the fine-tuning datasets. Specifically, we need to mask any mentions of brands or products to prevent bias. For instance, if a certain brand frequently appears in the training data within peak contexts, the model could erroneously link peak moments to that brand rather than learning the peak-language expressed in the text.\n\nThis precaution should extend to all aspects of our training data that might introduce biases. For example, as we now have examples from various projects, an overrepresentation of data from ‚Äògaming projects‚Äô in our ‚Äòpeak‚Äô posts within our training set (as opposed to the ‚Äòpit‚Äô posts) could skew the model into associating gaming-related language more with peaks than pits.\n\nBroadly the cleaning steps that should be applied to our data for finetuning are:\n\nMask brand/product mentions\nRemove hashtags #Ô∏è‚É£\nRemove mentions üí¨\nRemove URLs üåê\nRemove emojis üêô\n\n\n\n\n\n\n\nWhat about other cleaning steps?\n\n\n\n\n\nYou will notice here we do not remove stop words-. As explained in the previous step, part of the finetuning process is to finetune a sentence embedding model, and we want to keep stop words so that we can use the full context of the post in order to finetune accurate embeddings.\n\n\n\nAt this step, we can split out our data into training, testing, and validation datasets. A good rule of thumb is to split the data 70% to training data, 15% to testing data, and 15% to validation data. By default, SetFit oversamples the minimum class within the training data, so we shouldn‚Äôt have to worry too much about imbalanced datasets- though be aware if we have extreme imbalanced we will end up sampling the same contrastive pairs (normally positive pairs) multiple times. However, our experimentation has shown that class imbalance doesn‚Äôt seem to have a significant effect to the training/output of the SetFit model for peaks and pits.\nWe are now at the stage where we can actually fine-tune the model. There are many different parameters we can change when fine-tuning the model, such as the specific embedding model used, the number of epochs to train for, the number of contrastive pairs of sentences to train on etc. For more details, please refer to the Peaks and Pits Playbook\nWe can assess model performance on the testing dataset by looking at accuracy, precision, recall, and F1 scores. For peaks and pits, the most important metric is actually recall because in step 4 we reclassify posts using GPT, so we want to make sure we are able to provide as many true peak/pit moments as possible to this step, even if it means we also provide a few false positives.\n\n\n\n\n\n\nClick here for more info as to why recall is most important\n\n\n\n\n\nAs a refresher, precision is the proportion of positive identifications that are actually correct (it focuses on the correctness of positive predictions) whereas recall is the proportion of actual positives that are identified correctly (it focuses on capturing all relevant instances).\nIn cases where false positives need to be minimised (incorrectly predicting a non-event as an event) we need to prioritise precision - if you‚Äôve built a model to identify hot dogs from regular ol‚Äô doggos, high precision ensures that normal dogs are not misclassified as hot dogs.\nIn cases where false negatives need to be minimised (failing to detect an actual event) we need to prioritise recall - in medical diagnoses we need to minimise the number of times a patient is incorrectly told they do not have a disease when in reality they do (or worded differently, we need to ensure that all patients with a disease are identified).\nTo apply this to our problem- we want to be sure that we capture all (or as many as possible) relevant instances of peaks or pits- even if a few false positives come in (neither posts that are incorrectly classified as peaks or pits). As we use GPT to make further peak/pit identifications, it‚Äôs better to provide GPT with with a comprehensive set of potential peaks and pits, including some incorrect ones, than to miss out on critical data.\n\n\n\n\nVisualise model separation\nAs a bonus, we can actually neatly visualise how well our finetuning of the sentence transformer embedding model has gone- by seeing how well the model is able to separate our different classes in embedding space.\nWe can do this by visualising the 2-D structure of the embeddings and see how they cluster:\nThis is what it looks the embeddings space looks like on an un-finetuned model:\n\n\n\nUn-finetuned embedding model\n\n\nHere we can see that posts we know are peaks, pits, and neither all overlap and there is no real structure in the data. Any clustering of points observed are probably due to the posts‚Äô semantic similarity (c.f. the mode of transport example above). We would not be able to nicely use a classifier model to get a good mapping from this embedding space to our classes (i.e.¬†we couldn‚Äôt easily separate classes here).\nBy visualising the same posts after finetuning the embedding model, we get something more like this, where we can see that the embedding model now clearly separates posts based on their peak/pit classification (though we must be wary of overfitting!).\n\n\n\nFinetuned embedding model\n\n\nFinally, now we are happy with our model performance based on the training and validation datasets, we can evaluate the performance of this final model using our testing data. This is data that the model has never seen, and we are hoping that the accuracy and performance is similar to that of the validation data. This is Machine Learning 101 and if a refresher is needed for this there are plenty of resources online looking at the role of training, validation, and testing data.\n\n\n\n2.2.4 Run inference over project data (Step 4)\nIt is finally time to infer whether the project data contain peaks or pits by using our fine-tuned SetFit model to classify the posts.\nBefore doing this again we need to make sure we do some data cleaning on the project specific data.\nBroadly, this needs to match the high-level cleaning we did during fine-tuning stage:\n\nMask brand/product mentions (using RoBERTa-based model [or similar] and Rivendell functions)\nRemove hashtags #Ô∏è‚É£\nRemove mentions üí¨\nRemove URLs üåê\nRemove emojis üêô\n\n\n\n\n\n\n\nNote on social media sources\n\n\n\n\n\nCurrently all peak and pit projects have been done on Twitter or Reddit data, but if a project includes web/forum data quirky special characters, numbered usernames, structured quotes etc. should also be removed.\n\n\n\nOkay now we can finally run inference. This is extremely simple and only requires a couple of lines of code (again see the Peaks and Pits Playbook for code implementation)\n\n\n2.2.5 The metal detector, GPT-3.5 (Step 5)\nDuring step 4 we obtained peak and pit classification using few-shot classification with SetFit. The benefit of this approach (as outlined previously) is its speed and ability to classify with very few labelled samples due to contrastive learning.\nHowever, during our iterations of peak and pit projects, we‚Äôve realised that this step still classifies a fair amount of non-peak and pit posts incorrectly. This can cause noise in the downstream analyses and be very time consuming for us to further trudge through verbatims.\nAs such, the aim of this step is to further our confidence in our final list of peaks and pits to be actually peaks and pits. Remember before we explained that for SetFit, we focussed on recall being the most important measure in our business case? This is where we assume that GPT-3.5 enables us to remove the false positives due to it‚Äôs incredibly high performance.\n\n\n\n\n\n\nWhy not use GPT from the start?\n\n\n\n\n\nUsing GPT-3.5 for inference, even over relatively few posts as in peaks and pits, is expensive both in terms of time and money. Preliminary tests have suggested it is in the order of magnitude of thousands of times slower than SetFit. It is for these reasons why we do not use GPT-x models from the get go, despite it‚Äôs obvious incredible understanding of natural language.\n\n\n\nWhilst prompt-based classification such as those with GPT-3.5 certainly has its drawbacks (dependency on prompt quality, prompt injections in posts, handling and version control of complex prompts, unexpected updates to the model weights rendering prompts ineffective), the benefits include increased flexibility in what we can ask the model to do. As such, in the absence of an accurate, cheap, and quick model to perform span detection, we have found that often posts identified as peaks/pits did indeed use peak/pit language, but the context of the moment was not related to the brand/product at the core of the research project.\nFor example, take the post that we identified in the project 706, looking for peaks and pits relating to PowerPoint:\n\nThis brings me so much happiness! Being a non-binary graduate student in STEM academia can be challenging at times. Despite using my they/them pronouns during introductions, emails, powerpoint presentations, name tags, etc. my identity is continuously mistaken. Community is key!\n\nThis is clearly a ‚Äòpeak‚Äô, however it is not accurate or valid to attribute this memorable moment to PowerPoint. Indeed, PowerPoint is merely mentioned in the post, but is not a core driver of the Peak which relates to feeling connection and being part of a community. This is as much a PowerPoint Peak as it is a Peak for the use of emails.\nTherefore, we can engineer our prompt to include a caveat to say that the specific peak or pit moment must relate directly to the brand/product usage (if relevant).\n\n\n2.2.6 Topic modelling to make sense of our data (Step 6)\nNow we have an extremely refined set of posts classified as either peak or pits. The next step is to identify what these moments actually relate to (i.e.¬†identify the topics of these moments through statistical methods).\nTo do this, we employ topic modelling via BERTopic to identifying high-level topics that emerge within the peak and pit conversation. This is done separately for each product and peak/pit dataset (i.e.¬†there will be one BERTopic model for product A peaks, another BERTopic model for product A pits, an additional BERTopic model for product B peaks etc).\nWe implement BERTopic using the R package BertopicR. As there is already good documentation on BertopicR this section will not go into any technical detail in regards to implementation.\nFrom BertopicR. we end up with a topic label for each post in our dataset, meaning we can easily quantify the size of each topics and visualise temporal patterns of topic volume etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Peaks and Pits</span>"
    ]
  }
]