<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>conversation_landscape</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="conversation_landscape_files/libs/clipboard/clipboard.min.js"></script>
<script src="conversation_landscape_files/libs/quarto-html/quarto.js"></script>
<script src="conversation_landscape_files/libs/quarto-html/popper.min.js"></script>
<script src="conversation_landscape_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="conversation_landscape_files/libs/quarto-html/anchor.min.js"></script>
<link href="conversation_landscape_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="conversation_landscape_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="conversation_landscape_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="conversation_landscape_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="conversation_landscape_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="conversation-landscape" class="level1">
<h1>Conversation Landscape</h1>
<p>The ‚ÄòConversation Landscape‚Äô method has proven to be an effective tool for querying, auditing, and analyzing both broad concepts and finely grained topics across social conversations on all major platforms, as well as web pages and forums.</p>
<section id="project-background" class="level3">
<h3 class="anchored" data-anchor-id="project-background">Project Background</h3>
<p>Working with semi-structured or unstructured high-dimensional data, such as text (and in this case, social media posts), poses significant challenges in measuring or quantifying the language used to describe any specific phenomena. One common approach to quantifying language is topic modeling, where a corpus (or collection of documents and in this case, social media posts) is processed and represented in a simplified format. This often involves displaying top terms, verbatims, or threads highlighting any nuances or differences within the data. Traditional topic modeling or text analysis methods, such as Latent Dirichlet Allocation (LDA), operate on the probability or likelihood of terms or n-grams belonging to a set number of topics.</p>
<p>The Conversation Landscape workflow offers a slightly different solution and one that partitions our high-dimensional data without requiring data frames or burdening the user with sifting through rows of data to segment (or sense-check already segmented) data in order to understand or recognize differences across texts, which we will typically later define as topics. This is achieved through sentence transforming, where documents are converted from words to numerical values, which are often referred to as ‚Äòembeddings‚Äô. These values are calculated based on the content‚Äôs semantic and syntactic properties. The transformed values are then processed again using dimension reduction techniques, making the data more suitable for visualization. Typically, this involves reducing to two dimensions, though three dimensions may be used to introduce another layer of abstraction between our data points.</p>
<p>Note: This document will delve deeper into the core concepts of sentence transforming and dimension reduction, along with the different methods used to cluster or group topics once the overall landscape is mapped out. We will also take a closer look at an illustrated real-world business use case of this technique.</p>
<section id="final-output-of-project" class="level4">
<h4 class="anchored" data-anchor-id="final-output-of-project">Final output of project</h4>
<p>An ideal output would and should always showcase the positioning of our reduced data points onto the semantic space along with any topic or subtopic explanations with colour coding where appropriate. While sometimes we provide raw counts of documents per topic and subtopic, we always include the % of topic distribution across our corpus, which is occasionally referred to as Share of Voice(SOV).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="/img/ai_landscape_output_example.png" class="img-fluid figure-img"></p>
<figcaption>Screenshot Taken from the Final Output of an AI Landscape Microsoft Project - Q2 FY24</figcaption>
</figure>
</div>
</section>
</section>
<section id="how-to-get-there" class="level2">
<h2 class="anchored" data-anchor-id="how-to-get-there">How to get there</h2>
<p>As promised, we will provide some more context as well as the appropriate information surrounding the required steps taken, so that a reader may replicate and implement the methods mentioned throughout so far, providing an efficient analysis tool for any set of documents or corpus regardless of domain specifics. While the output provided above showcases a simplified means for visualizing complex and multifaceted noisy data such as the ‚ÄòArtificial Intelligence‚Äô conversation on social, there are a number of steps that one must take and be mindful of throughout in order to create the best fit model and the optimal output of a Conversational Landscape project.</p>
<p>The steps would include, and as one might find in any project especially within the realms of Natural Language Processing (NLP); Initial Exploratory Data Analysis (check the data is relevant and fit to answer the brief), Cleaning and Processing (the removal of spam, unhelpful or irrelevant data and pre-processing of text variable for embedding), Transforming/Embedding (turning our words to numbers), Dimension Reduction (reducing our representational values of documents to a manageable state in order to visualise) and then Topic model/Clustering (where we scientifically model and define our data).</p>
<section id="exploratory-data-analysis-eda" class="level4">
<h4 class="anchored" data-anchor-id="exploratory-data-analysis-eda">Exploratory Data Analysis (EDA):</h4>
<p>As we are not normally responsible for querying, sourcing or collecting our own data, the first steps in our workflow should always involve some high-level data checks before we proceed with any of the following steps in order to save time downstream and give us confidence to carry over into the data cleaning and processing process.</p>
<p>First, check things like the existing variables and clean or rename any where necessary. This step requires a little forward thinking as to what columns are necessary to complete each stage of the project. Once happy with our variables, we can then check for missing values such as missing dates or days of data, any abnormal distributions across columns like Social Platform that might skew any findings or help to explain/justify the resulting topic model at the end of the workflow. We can then do some bespoke or project specific checks like searching for likely-to-find strings or terms within our text variable to ensure the data is relevant and query had captured the phenomena we are aiming to model.</p>
</section>
<section id="data-cleaningprocessing" class="level4">
<h4 class="anchored" data-anchor-id="data-cleaningprocessing">Data Cleaning/Processing:</h4>
<p>Again, as we are not responsible for data collection and as we might expect, our data may contain unhelpful or even problematic information that is unwillingly bought in by the query. Our job at this stage is to minimize the amount of spam or unhelpful data existing in our corpus to ensure our findings are accurate and appropriate, and represent the overall data and concept of the conversation that we are modelling.</p>
<p>The optimal procedures for spam detection and removal are covered in more detail <em>here(will include link when section complete)</em>. However, when conducting methods such as these, there are steps one absolutely must take to ensure that the text variable provided to the sentence transformer model is clean and concise so that an accurate and consistent embedding process can take place on our documents. This includes the removal of:</p>
<ul>
<li>Hashtags #Ô∏è‚É£</li>
<li>User/Account Mentions üí¨</li>
<li>URLs or Links üåê</li>
<li>Emojis üêô</li>
<li>Non-English Characters üâê</li>
</ul>
<p>Often, one might also choose to remove punctuation and/or digits, however in our provided example, we have not done so. There are also things to beware of such as documents beginning with numbers that can influence the later processes so unless necessary, we should remove these where possible to ensure no inappropriate grouping of documents takes place based on these minor similarities, as it‚Äôs the semantic and pure essence of posts that we want to mainly focus on in this case, grouping similarly natured documents together to form clusters(topics/subtopics).</p>
</section>
<section id="sentence-transformingembedding" class="level4">
<h4 class="anchored" data-anchor-id="sentence-transformingembedding">Sentence Transforming/Embedding:</h4>
<p>Once we are happy with the cleanliness and relevance of our data, as well as the text variable we wish to analyse, we can begin embedding our documents so that we have a numerical representation for each that can later be reduced and visualized. Typically, and in this case we leverage pre-trained sentence transformer models hosted on Hugging Face, such as <code>all-mpnet-base-v2</code> which we had decided to use for the Microsoft AI project, as it had great performance scores for how lightweight it is, however there are similar models now posting more competitively across these metrics, so one may wish to consult the <a href="https://huggingface.co/spaces/mteb/leaderboard">Hugging Face leaderboard</a> or simply do some desk research before settling on a model appropriate for their own specific use case.</p>
<p>While the previous steps might have been taken using R and Rstudio making use of SHARE‚Äôs suite of data cleaning, processing and parsing functionality, the embedding process will need to be completed using Google Colab to take advantage of their premium GPUs and high RAM option as embedding requires a large amount of compute, so much so that most fairly competent machines with standard tech specs will struggle. It is also worth noting that an embedding output may depend on the specific GPU being utilised as well as the version of Python that Colab is currently running, its good practice to make note of both of these (you may thank yourself at a later stage). To get going with sentence transformers and for downloading/importing a model such as <a href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2">all-mpnet-bas-v2</a>.</p>
</section>
<section id="dimension-reduction" class="level4">
<h4 class="anchored" data-anchor-id="dimension-reduction">Dimension Reduction:</h4>
<p>At this stage, we would expect to have our cleaned data and its representative embedding output. This next step, and one responsible for taking this high-dimensional object(often 768 columns and some embedding models output a lot more) to then simplify/reduce these columns down enough to map our documents onto a semantic space where they are represented as a node and positioned based upon their nature, meaning those closer semantically will be closer together upon our abstract space, forming our landscape.</p>
<p>There are a number of ways one can process an embeddings output to produce either a 2 or 3 dimensional representation of data. Each method has its own merits as well as appropriate use cases, which mostly depend whether the user intends to focus on either the local or global structure of their data. For a bit more on the different dimension reduction techniques, the <a href="https://maartengr.github.io/BERTopic/getting_started/dim_reduction/dim_reduction.html">BERTopic documentation</a> provides some further detail while staying relevant to the subject of Topic Modelling in NLP.</p>
<p>Once we have reduced out embeddings, and in this instance we had used Uniform Manifold Approximation and Projection(UMAP) as it is useful when being mindful of both the local and global structures within data, and reduced to just two dimensions we should be readu to formulate our 2D plot such as the one below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="/img/ai_landscape_grey.png" class="img-fluid figure-img"></p>
<figcaption>Grey Colourless Landscape Plot from an AI Landscape Microsoft Project - Q2 FY24</figcaption>
</figure>
</div>
</section>
<section id="topic-modellingclustering" class="level4">
<h4 class="anchored" data-anchor-id="topic-modellingclustering">Topic Modelling/Clustering:</h4>
<p>Information regarding the topic modelling step and different options etc etc, how we go from grey landscape to segmented‚Ä¶</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="/img/ai_landscape_coloured.png" class="img-fluid figure-img"></p>
<figcaption>Segmented Colourful Landscape Plot from an AI Landscape Microsoft Project - Q2 FY24</figcaption>
</figure>
</div>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>